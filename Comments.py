# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p1-SHmgU4qcN7cOCLGN7Hk6PRO9zdqzV
"""

!pip install selenium
!pip install wordcloud

import re
import time
from tqdm import tqdm

import os
import googleapiclient.discovery

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from selenium import webdriver

import nltk
from nltk.corpus import stopwords as nltk_stopwords
from pymystem3 import Mystem
from wordcloud import WordCloud

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import *

"""# Парсинг комментариев с YouTube"""

DEVELOPER_KEY = "" # добавить сюда ключ от google api
VIDEO_ID = "JjGEBCTmg2g"

# Функция для скачивания корневых комментариев
def youtube(nextPageToken=None):
    # Disable OAuthlib's HTTPS verification when running locally.
    # *DO NOT* leave this option enabled in production.
    os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "1"

    api_service_name = "youtube"
    api_version = "v3"

    youtube = googleapiclient.discovery.build(
        api_service_name, api_version, developerKey=DEVELOPER_KEY)

    request = youtube.commentThreads().list(
        part="id,snippet",
        maxResults=100,
        pageToken=nextPageToken,
        videoId=VIDEO_ID
    )
    response = request.execute()
    return response

  # Функция для скачивания реплаев на комментарии
def youtubechild(NextParentId, nextPageToken=None):
    # Disable OAuthlib's HTTPS verification when running locally.
    # *DO NOT* leave this option enabled in production.
    os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "1"

    api_service_name = "youtube"
    api_version = "v3"

    youtube = googleapiclient.discovery.build(
        api_service_name, api_version, developerKey=DEVELOPER_KEY)

    request = youtube.comments().list(
        part="id,snippet",
        maxResults=100,
        pageToken=nextPageToken,
        parentId=NextParentId
    )
    response = request.execute()
    return response

# Главная функция
def mainParseComments():
    # Скачиваем комментарии
    print('download comments')
    response = youtube()
    items = response.get("items")
    nextPageToken = response.get("nextPageToken")  # скачивается порциями, на каждую следующую выдаётся указатель
    i = 1
    while nextPageToken is not None:
        print(str(i * 100))  # показываем какая сотня комментариев сейчас скачивается
        response = youtube(nextPageToken)
        nextPageToken = response.get("nextPageToken")
        items = items + response.get("items")
        if i * 100 == 20000:
            break
        i += 1

    print(f"Final count: {len(items)}")  # Отображаем количество скачаных комментариев

    # Делаем датафрейм из полученной инфы
    comments = [line.get("snippet").get("topLevelComment").get('snippet').get('textOriginal') for line in items]
    comments_df = pd.DataFrame({'comment': comments})
    comments_df.to_csv('youtuberesults.csv')

    print("done")

    return comments_df

comments_df = mainParseComments()

"""# Загрузка и обработка размеченного *датасета*"""

# Зададим путь к основной папке
path_main = r"C:\Users\gleb9\OneDrive\Рабочий стол\UNI\diploma\parsingYouTubeComments"

positive = pd.read_csv('positive.csv',
                       sep=';',
                       header=None
                       )

negative = pd.read_csv('negative.csv',
                       sep=';',
                       header=None
                       )

positive_text = pd.DataFrame(positive.iloc[:, 3])
negative_text = pd.DataFrame(negative.iloc[:, 3])

positive_text['label'] = [1] * positive_text.shape[0]
negative_text['label'] = [0] * negative_text.shape[0]

labeled_tweets = pd.concat([positive_text, negative_text])

labeled_tweets.index = range(labeled_tweets.shape[0])

labeled_tweets.columns = ['text', 'label']

labeled_tweets

"""# Очистка размеченного датасета"""

# Оставим в тексте только кириллические символы
def clear_text(text):
    clear_text = re.sub(r'[^А-яЁё]+', ' ', text).lower()
    return " ".join(clear_text.split())


# Напишем функцию удаляющую стоп-слова
def clean_stop_words(text, stopwords):
    text = [word for word in text.split() if word not in stopwords]
    return " ".join(text)

# Загрузим список стоп-слов
nltk.download('stopwords')
stopwords = set(nltk_stopwords.words('russian'))
np.array(stopwords)

# Протестируем работу функции очистки текста
text = labeled_tweets['text'][np.random.randint(labeled_tweets.shape[0])]
print(text)
print('=======================================')
print(clean_stop_words((clear_text(text)), stopwords))

start_clean = time.time()

labeled_tweets['text_clear'] = labeled_tweets['text'].apply(lambda x: clean_stop_words(clear_text(str(x)), stopwords))

print('Обработка текстов заняла: ' + str(round(time.time() - start_clean, 2)) + ' секунд')

labeled_tweets = labeled_tweets[['text_clear', 'label']]
labeled_tweets.to_csv(path_main + 'labeled_tweets_clean.csv')

"""# Получение TF-IDF векторных представлений размеченных текстов

"""

# Для обучения классификатора получим значения IDF (inference document frequency) для слов из тренировочного набора данных, 
# значения IDF равны логарифму отношения количества документов к количеству документов содержащих искомое слово. 
# Например для стандартных слов, которые встречаются практически в любом тексте IDF будет близок к единице, 
# а для специфичных, которые встречаются в одном тексте из 100 это значение будет равно уже 2 (если мы берем основание логарифма 10).
# Затем получив словарь со значениями IDF мы можем получить векторное представление каждого текста по следующему принципу, значения IDF слова умножаем на значения

labeled_tweets.columns = ['text', 'label']

# предварительно разделим выборку на тестовую и обучающую
train, test = train_test_split(labeled_tweets,
                               test_size=0.2,
                               random_state=12348,
                               )

print(train.shape)
print(test.shape)

# Сравним распределение целевого признака
for sample in [train, test]:
    print(sample[sample['label'] == 1].shape[0] / sample.shape[0])

"""# Предварительное обучение моделей"""

# Получим векторные представления текстов
count_idf_1 = TfidfVectorizer(ngram_range=(1, 1))

tf_idf_train_base_1 = count_idf_1.fit_transform(train['text'])
tf_idf_test_base_1 = count_idf_1.transform(test['text'])

print(tf_idf_test_base_1.shape)
print(tf_idf_train_base_1.shape)

sample = test.sample(n=1)['text']
sample_tf_idf = count_idf_1.transform(sample)

print(sample_tf_idf.shape)

array = sample_tf_idf.toarray()
print(array)

print(sample)

print(array[array != 0])  # конкретные слова, их значения idf умноженные на их количество встречаемости в этом тексте

"""# Логистическая регрессия"""

model_lr_base_1 = LogisticRegression(solver='lbfgs',
                                     random_state=12345,
                                     max_iter=10000,
                                     n_jobs=-1)

# """Получим прогноз и оценим качество модели"""

model_lr_base_1.fit(tf_idf_train_base_1, train['label'])

predict_lr_base_proba = model_lr_base_1.predict_proba(tf_idf_test_base_1)

# Два столбца, первые вероятность того, что текст отрицательный и вероятность того, что текст положительный
# Если пары сложить, то мы получим большой набор единиц

print(predict_lr_base_proba)
print()
print(predict_lr_base_proba.sum(axis=1))

# """В качестве сравнения сделаем классификатор который в качестве прогноза выдает случайное число в интервале от 0 до 1 """

def coin_classifier(X: np.array) -> np.array:
    predict = np.random.uniform(0.0, 1.0, X.shape[0])
    return predict

coin_predict = coin_classifier(tf_idf_test_base_1)

fif = plt.figure(figsize=(8, 6))

pd.Series(coin_predict) \
    .hist(bins=100,
          alpha=0.7,
          color='r',
          label='Coin'
          )

pd.Series(predict_lr_base_proba[:, 1]) \
    .hist(bins=100,
          alpha=0.7,
          color='b',
          label='TF-IDF LogisticRegression'
          )
plt.legend()
plt.show()

"""# Визуализация ROC-кривых классификаторов"""

fpr_base, tpr_base, _ = roc_curve(test['label'], predict_lr_base_proba[:, 1])
roc_auc_base = auc(fpr_base, tpr_base)

fpr_coin, tpr_coin, _ = roc_curve(test['label'], coin_predict)
roc_auc_coin = auc(fpr_base, tpr_base)

fig = make_subplots(1, 1,
                    subplot_titles=["Receiver operating characteristic"],
                    x_title="False Positive Rate",
                    y_title="True Positive Rate"
                    )

fig.add_trace(go.Scatter(
    x=fpr_base,
    y=tpr_base,
    fill = 'tozeroy',
    name="ROC base (area = %0.3f)" % roc_auc_base,
))

fig.add_trace(go.Scatter(
    x=fpr_coin,
    y=tpr_coin,
    mode='lines',
    line=dict(dash='dash'),
    name='Coin classifier (area = 0.5)'
))

fig.update_layout(
    height=600,
    width=800,
    xaxis_showgrid=False,
    xaxis_zeroline=False,
    template='plotly_dark',
    font_color='rgba(212, 210, 210, 1)'
)

"""# Матрицы ошибок"""

# Выведем матрицы ошибок
confusion_matrix(test['label'],
                 (predict_lr_base_proba[:, 1] > 0.5).astype('float'),
                 normalize='true',
                 )
# [[true-neg, false-neg(доля отрицательных, к-е квалифицировались как положительные)],отрицательные 
# [false-pos(доля положительных, к-е квалифицировались как отрицательные), true-pos]] положительные

"""# Визуализация важности признаков"""

# Получим веса признаков, то есть множители
# подобранные логистической регрессией
# для каждого компонента вектора tf-idf

weights = pd.DataFrame({'words': count_idf_1.get_feature_names_out(), # get_feature_names_out - веса из классификатора
                        'weights': model_lr_base_1.coef_.flatten()}) # coef_ - кожффициенты из лог регрессии
weights_min = weights.sort_values(by='weights')
weights_max = weights.sort_values(by='weights', ascending=False)

# сортировка по убыванию
weights_min = weights_min[:100]
weights_min['weights'] = weights_min['weights'] * -1
print(weights_min)

# сортировка по возрастанию
weights_max = weights_max[:100]
print(weights_max)

"""# Облако тегов"""

# Воспользуемся библиотекой wordcloud для генерации картинок

wordcloud_positive = WordCloud(background_color="white",
                               colormap='Blues',
                               max_words=200,
                               mask=None,
                               width=1600,
                               height=1600) \
                            .generate_from_frequencies(
                            dict(weights_max.values))

wordcloud_negative = WordCloud(background_color="black",
                               colormap='Reds',
                               max_words=200,
                               mask=None,
                               width=1600,
                               height=1600) \
                            .generate_from_frequencies(
                            dict(weights_min.values))

# Выведем картинки сгенерированные вордклаудом
fig, ax = plt.subplots(1, 2, figsize=(20, 12))

ax[0].imshow(wordcloud_positive, interpolation='bilinear')
ax[1].imshow(wordcloud_negative, interpolation='bilinear')

ax[0].set_title('Топ ' + \
                str(weights_max.shape[0]) + \
                ' слов\n с наибольшим положительным весом',
                fontsize=20
                )
ax[1].set_title('Топ ' + \
                str(weights_min.shape[0]) + \
                ' слов\n с наибольшим отрицательным весом',
                fontsize=20
                )

ax[0].axis("off")
ax[1].axis("off")

plt.show()

"""# Снижение размерности признакового пространства модели"""

fig = make_subplots(1, 1)

fig.add_trace(go.Histogram(
    x=weights.query('weights != 0')['weights'],
    # histnorm = 'probability',
    opacity=0.5,
    showlegend=False
))

fig.add_trace(go.Histogram(
    x=weights.query('weights > 0.25 or weights < -0.25')['weights'],
    # histnorm = 'probability',
    opacity=0.5,
    showlegend=False
))

fig.update_layout(
    height=600,
    width=800,
    xaxis_showgrid=False,
    xaxis_zeroline=False,
    template='plotly_dark',
    font_color='rgba(212, 210, 210, 1)'

)

# График, по к-ому видно, что большинство слов имеют вес довольно близкий к нулю
# очень маленькая часть слов имеет вес < 0.5 или > 0.5

# отсечем тексты, которые не особо влияют на ситуацию
# может качество не ухудшится, но хотя бы уменьим количество необходимых вычислений
vocab = weights.query('weights > 0.25 or weights < -0.25')['words']

print(vocab)

"""# Получим новые векторные представления текстов"""

count_idf = TfidfVectorizer(vocabulary=vocab,
                            ngram_range=(1, 1))

tf_idf_train = count_idf.fit_transform(train['text'])
tf_idf_test = count_idf.transform(test['text'])

print(tf_idf_test.shape)
print(tf_idf_train.shape)

# Заново обучаем логическую регрессию на обучающем наборе данных train
model_lr_base = LogisticRegression(solver='lbfgs',
                                   random_state=12345,
                                   max_iter=10000,
                                   n_jobs=-1)

model_lr_base.fit(tf_idf_train, train['label'])

"""# Получим прогноз и оценим качество модели"""

predict_lr_base_proba_1 = model_lr_base.predict_proba(tf_idf_test) # прогноз на тестовом наборе данных

# Оценим качество классификации
fpr_base_1, tpr_base_1, _ = roc_curve(test['label'], predict_lr_base_proba_1[:, 1])
roc_auc_base_1 = auc(fpr_base_1, tpr_base_1)

# Построим ROC-кривую но основе новых данных
fig = make_subplots(1, 1,
                    subplot_titles=["Receiver operating characteristic"],
                    x_title="False Positive Rate",
                    y_title="True Positive Rate"
                    )

fig.add_trace(go.Scatter(
    x=fpr_base,
    y=tpr_base,
    fill='tozeroy',
    name="ROC curve (area = %0.3f)" % roc_auc_base,
))

fig.add_trace(go.Scatter(
    x=fpr_base_1,
    y=tpr_base_1,
    fill='tozeroy',
    name="Less dimensity ROC curve (area = %0.3f)" % roc_auc_base_1,
))

fig.add_trace(go.Scatter(
    x=fpr_coin,
    y=tpr_coin,
    mode='lines',
    line=dict(dash='dash'),
    name='Coin classifier'
))

fig.update_layout(
    height=600,
    width=800,
    xaxis_showgrid=False,
    xaxis_zeroline=False,
    template='plotly_dark',
    font_color='rgba(212, 210, 210, 1)'
)

"""# Вывод"""

# Мы снизили размерность векторов tf-idf потеряв при этом 0.2% качества 
# (площадь под ROC кривой при размерности > 170К 0.808, 
# площадь под ROC кривой при размерности > 48K -- 0.806)
# Вполне адекватная цена за снижение размерности примерно в 3 раза

"""# Подбор оптимального порогового значения классификации"""

scores = {}

weight = 0.535 # небольшой перекос веса в сторону негативных комментариев
# но в то же время максимизируем сумму правильно угаданных негативных и позитивных

for threshold in np.linspace(0, 1, 100):
    matrix = confusion_matrix(test['label'],
                              (predict_lr_base_proba[:, 0] < threshold).astype('float'),
                              normalize='true',
                              )

    score = matrix[0, 0] * weight + matrix[1, 1] * (1 - weight)

    scores[threshold] = score

threshold_df = pd.DataFrame({'true_score': scores.values(),
              'threshold': scores.keys()},
             ).sort_values(by='true_score', ascending=False)

# list(threshold_df[['threshold']])
threshold = float(list(dict(threshold_df)['threshold'])[0])

# матрица ошибок с выбранным проговым значением
matrix = confusion_matrix(test['label'],
                          (predict_lr_base_proba[:, 0] < threshold).astype('int'),
                          normalize='true',
                          )

matrix

"""# Классификация не размеченных комментариев"""

# C помощью обученных tf-idf векторизатора и логистической регрессии получим оценки вероятности негатива в каждом из комментариев

# Очистим тексты комментариев под видео
start_clean = time.time()

comments_df['text_clear'] = comments_df['comment'].apply(lambda x: clean_stop_words(clear_text(str(x)), stopwords))

print('Обработка текстов заняла: ' + str(round(time.time() - start_clean, 2)) + ' секунд')

"""# Визуализация ключевых слов"""

video_counter = CountVectorizer(ngram_range=(1, 1))
video_count = video_counter.fit_transform(comments_df['text_clear'])

print(video_count.toarray().sum(axis=0).shape)

video_counter.get_feature_names_out().shape

# Сохраним списки Idf для каждого класса

video_frequence = pd.DataFrame(
    {'word': video_counter.get_feature_names_out(),
     'frequency': video_count.toarray().sum(axis=0)
     }).sort_values(by='frequency', ascending=False)

print(video_frequence.shape[0])

# Воспользуемся библиотекой wordcloud для генерации картинок

wordcloud_video = WordCloud(background_color="black",
                              colormap='Blues',
                              max_words=200,
                              mask=None,
                              width=1600,
                              height=1600) \
                            .generate_from_frequencies(
                            dict(video_frequence.values))

# Выведем картинки сгенерированные вордклаудом
fig, ax = plt.subplots(1, 1, figsize=(20, 12))

ax.imshow(wordcloud_video, interpolation='bilinear')

ax.set_title('Топ ' + \
                str(video_frequence.shape[0]) + \
                ' слов наиболее уникальных слов,\n ',
                fontsize=20
                )

ax.axis("off")
plt.show()

"""# Получение оценки негативности комментария"""

comments_df

# Выведем 5 случайных комментариев c оценкой негатива видео
for _ in range(5):
    source = comments_df.sample(n=1)
    text_clear = source['text_clear'].values[0]
    text = source['comment'].values[0]

    print(text)

    tf_idf_text = count_idf.transform([text_clear])

    toxic_proba = model_lr_base.predict_proba(tf_idf_text)

    print('Вероятность негатива: ', toxic_proba[:, 0])
    print()

# Получим оценки негатива для всех комментариев

video_tf_idf = count_idf.transform(comments_df['text_clear'])
video_negative_proba = model_lr_base.predict_proba(video_tf_idf)
comments_df['negative_proba'] = video_negative_proba[:, 0]

"""# Найдем доли негативных комментариев при оптимальном пороговом значении"""

# Выводим среднее значение
video_share_neg = (comments_df['negative_proba'] > threshold).sum() / comments_df.shape[0]

video_share_neg

"""# Гистограмма оценки комментариев под видео"""

fig = make_subplots(1, 1,
                    subplot_titles=['Распределение комментариев по оценке негативности']
                    )

fig.add_trace(go.Violin(
    x=comments_df['negative_proba'],
    meanline_visible=True,
    name='Video (N = %i)' % comments_df.shape[0],
    side='positive',
    spanmode='hard'
))

fig.add_annotation(x=0.8, y=1.5,
                    text=f"%0.2f — доля негативных комментариев (при p > {round(threshold, 2)})" % video_share_neg,
                    showarrow = False,
                    yshift = 10)

fig.update_traces(orientation='h',
                  width=1.5,
                  points=False
                  )

fig.update_layout(height=500,
                  # xaxis_showgrid=False,
                  xaxis_zeroline=False,
                  template='plotly_dark',
                  font_color='rgba(212, 210, 210, 1)',
                  legend=dict(
                      y=0.9,
                      x=-0.1,
                      yanchor='top',
                  ),
                  )
fig.update_yaxes(visible=False)

fig.show()